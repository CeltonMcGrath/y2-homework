\documentclass[letterpaper,10pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{fullpage}
\usepackage{enumerate}
\usepackage{setspace}
\usepackage{ amssymb }
\usepackage{mathtools}
\usepackage{fancyhdr}
\usepackage{algpseudocode}
\pagestyle{fancy}

\lhead{Assignment 2 - CSC265}
\rhead{David Knott, \#999817685}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% http://www.maths.tcd.ie/~dwilkins/LaTeXPrimer/Theorems.html
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}


% \def\pball{\mathrel{%
%     \mathchoice{\PBALL}{\PBALL}{\scriptsize\PBALL}{\tiny\PBALL}%
% }}
% \def\PBALL{{%
%     \setbox0\hbox{B}%
%     \rlap{\hbox to \wd0{\hss|\hss}}\box0
% }}

\allowdisplaybreaks
\newcommand{\tinyquot}[1]{\begin{center}{\footnotesize #1}\end{center}}
\newcommand*{\QEDA}{\hfill\ensuremath{\blacksquare}}
\addtolength{\belowdisplayskip}{-25mm}
\begin{document}
\title{Assignment 2 - CSC265}
\author{David Knott \\  Student \#999817685}
\date{October 21, 2013}
\maketitle
\begin{enumerate}
	\item 

	\item 
	\begin{enumerate}[a)]
		\item Note that for any $h \in \mathcal{H}$ for all distinct $x_1,x_2,x_3 \in U$ and all $y_1,y_2,y_3 \in \{0,\dots,m-1\}$ we have that:
		\begin{align*}
		\text{Prob}_{h\in\mathcal{H}}[ h(x_1) = y_1 \wedge h(x_2) = y_2 \wedge h(x_3) = y_3 ] & = \\
		\text{Prob}_{h\in\mathcal{H}}[h(x_1) = y_1] & \times \\
		\text{Prob}_{h\in\mathcal{H}}[h(x_2) = y_2] & \times \\
		\text{Prob}_{h\in\mathcal{H}}[h(x_3) = y_3] & = \frac{1}{m^3}
		\end{align*}
		Since the $x$s are arbitrary, all of these probabilities must be the same. Solving for each probability yields $1/m$. So for for all distinct $x_1,x_2,x_3 \in U$ and all $y_1,y_2,y_3 \in \{0,\dots,m-1\}$ we have that 
		\begin{align*}
		\text{Prob}_{h\in\mathcal{H}}[h(x_1) = y_1] & = \frac{1}{m} \\
		\text{Prob}_{h\in\mathcal{H}}[h(x_2) = y_2] & = \frac{1}{m} \\
		\text{Prob}_{h\in\mathcal{H}}[h(x_3) = y_3] & = \frac{1}{m}
		\end{align*}
		If we let $y_1 = h(x_2)$, since $h(x_2) \in \{0,\dots,m-1 \}$ we have:
		$$ \text{Prob}_{h\in\mathcal{H}}[h(x_1) = h(x_2)] = \frac{1}{m} $$
		Which implies that a 3-universal family is universal.
		% \item Because $m$ is prime, the function $h_{a,b,c}$ is a polynomial in a finite field. For any $h_{a,b,c} \in \mathcal{H}$ then given $x \in U$ for any $y \in U$ if $h_{a,b,c}(x) = y$ then $a x^2 + b x + r \mod m = 0$ where $r = c - y \mod m$.
	\end{enumerate}
	\clearpage

	\item
	\begin{enumerate}[a)]
		\item At each node we store the number of nodes and the average of the subtree rooted at that node.

		\item While we're inserting the item we traverse down the tree to find a spot to put the new node. At every node we pass, we add one to the subtree total and recompute the average with the formula $(a + nc)/(n+1)$ where $a$ is the value we're inserting, $c$ was the original average and $n$ was the original number of nodes in the subtree. During rotations involved nodes recompute their subtree averages with similar formulas. The amount of nodes that need recomputing is constant, since it only happens when a node's children move out of their subtree. For any rotation the number of nodes that have this happen is at most 2. This means the running time is unaffected.

		\item
	\end{enumerate}
	\clearpage

	\item

	\begin{enumerate}[a)]
		\item Since each insert increases the size of the list by 1, we must find the expected number of prepends after $k-1$ operations. Using the indicator random variable $I_{n}$ associated with the event that the $n$th operation was a prepend. If $I$ is the random variable for the number of prepends after $k-1$ operations then we have:
		\begin{align*}
			I & = \sum^{k-1}_{n=1} I_n \implies \\
			\text{E}\big[I\big] & = \text{E}\Bigg[ \sum^{k-1}_{n=1} I_n \Bigg] \implies \\
			\text{E}\big[I\big] & = \sum^{k-1}_{n=1} \text{E}\big[ I_n \big] \implies \\
			\text{E}\big[I\big] & = \sum^{k-1}_{n=1} p \implies \\
			\text{E}\big[I\big] & =  (k-1)p
		\end{align*}
		By way of the linearity of expectation an the equality $\text{E}[I\{A\}] = \text{Pr}\{A\}$.
		The expected length of the list after $k-1$ operations is $(k-1)p$.
		\item At this point the expected size of the list is $(k-1)p$. Assuming that the $k$th operation is an access, the number of expected number of steps would be:
		\begin{align*}
			\text{E}\big[I\big] & = \sum^{|S|}_{n=1} \frac{1}{|S|} n \implies \\
			\text{E}\big[I\big] & = \sum^{(k-1)p}_{n=1} \frac{n}{(k-1)p} \implies \\
			\text{E}\big[I\big] & = \frac{1}{2} ((k-1)p + 1)
		\end{align*}
		Where $I$ is the random variable for the number of steps given the $k$th operation is an access.

		If the $k$th operation is a prepend then the number of steps is 1. Let $A$ be the event that the operation is a prepend and $B$ be the event that the operation is an access. If $J_k$ is the random variable for the number of steps at the $k$th operation we have: 
		\begin{align*}
			\text{E}\big[J_k\big] & = \text{Pr}(A) + \text{Pr}(B)\frac{1}{2} ((k-1)p + 1)  \implies \\
			\text{E}\big[J_k\big] & = p + (1-p)\frac{1}{2} ((k-1)p + 1)  \implies \\
			\text{E}\big[J_k\big] & = \frac{1}{2} (-k p^2+k p+p^2+1) 
		\end{align*}
		So the expected number of steps is $\frac{1}{2} (-k p^2+k p+p^2+1) $
		\item If $J$ is the random variable for the number of steps for the entire series of operations, then we have:
		\begin{align*}
			J & = \sum^{n}_{k=1} J_k \implies \\
			\text{E}\big[J\big] & = \text{E}\Bigg[ \sum^{n}_{k=1} J_k \Bigg] \implies \\
			\text{E}\big[J\big] & = \sum^{n}_{k=1} \text{E}\big[ J_k \big] \implies \\
			\text{E}\big[J\big] & = \sum^{n}_{k=1} \frac{1}{2} (-k p^2+k p+p^2+1)  \implies \\
			\text{E}\big[J\big] & = \frac{1}{4} n (2+p+n p+p^2-n p^2)
		\end{align*}
		So $n$ operations is expected to take $\frac{1}{4} n (2+p+n p+p^2-n p^2)$ steps.
	\end{enumerate}
	\clearpage

	\item

	\begin{enumerate}[a)]
		\item 
		\begin{enumerate}[i.]
			\item Because the black height property implies that the tree has a sense of symmetry, the black hight can only change if a rotation occurs at the root node. In these cases, since the number of nodes affected by the rotation is minimal, recalculating the black height shouldn't be to difficult. If we only compute black height when it's possible to do it once in constant time (or at the same time another constant time operation would be running), then the running time of the insert and delete algorithms will not change.

			While descending down T we can keep track of the black height by subtracting one from the black-height for every back node we traverse away from. 
			\item We can simply traverse as far to the right as possible until we reach the black height of $bh[T_2]$. We keep track of the black height using the method mentioned in i.
			\item We let $x$ be the root of the new subtree and have $T_y$ and $T_2$ hang off the left and right of $x$. Since $key[x_1] \leq key[x_2]$ for any $x_1 \in T_y$ and $x_2 \in T_2$ the subtree will maintain the binary search tree property.
			\item If we let $x$ be a red node then we if $x$'s parent is red we do the bubble up recoloring procedure to keep the red-black property of the tree maintained. This will take $\log n$ time.
			\item When this is the case we just swap $T_1$ and $T_2$.
			\item Since the worst operation we have to preform is the recoloring which takes $O(\log n)$ time, the algorithm has a running time of $O(\log n)$
		\end{enumerate}
	\end{enumerate}
	\clearpage
	\item 
		For a list of lists with these restrictions, the number of lists and the maximum size of the largest list will be:
		$$ \ceil{ \frac{1}{2} (\sqrt{8n + 1} - 1)} \in O(\sqrt{n}) $$
		In addition to this data structure, each group will keep track of how ``rotated'' their list is. This way we can find the maximum and minimum elements of an array in constant time. Keeping this information updated will not add anything to the runtime of any of the algorithms. For convenience, if $g$ is a subgroup of $S$, then $g.\max$ and $g.\min$ will yield the maximum and minimum elements.

	\begin{enumerate}[a)]
		\item For insertion, for each step in the algorithm we maintain the property that the $k^{th}$ group has at most $k$ elements. This means every time we insert an element into a full group we must take one out. Taking advantage of this fact, consider the following sub algorithm:

		\begin{algorithmic}
		\Function{INSERT-INTO-GROUP}{$g$, $x$}
			\If{$x \geq g.\max$}
				\State \Return $x$
			\EndIf

			\State $r \gets g.\max$
			\State Put $x$ where $g.\max$ was.
			\State Subtract one to the group's rotation amount.
			\State Bubble $x$ up the list until it reaches equilibrium.
			\State \Return $r$

		\EndFunction
		\end{algorithmic}
		Since all the elements of a group are smaller than any of the elements in it's successor, the return value of this algorithm will be the minimum of the next group if we inserted it there. This is precisely how the algorithm will work: 
		\begin{algorithmic}
		\Function{INSERT}{$S$, $x$}
			\State $i \gets 1$
			\State $r \gets x$
			\While{$i < |S|$}
				\State $r \gets \text{INSERT-INTO-GROUP}(S[i], r)$
			\EndWhile

			\If{$S[|S|]$ is full}
				\State $S[|S|+1] \gets [r]$
			\Else
				\State Put $r$ at the end of $S[|S|]$ and bubble until equilibrium.
			\EndIf
		\EndFunction
		\end{algorithmic}
		The subfunction INSERT-INTO-GROUP has two behaviors: the first will return $x$ without affecting $g$, the second will return the maximum value of $g$, having inserted $x$ into $g$ by preforming at most $|g|$ shifting operations. If $x$ was already the minimum of $g$ then there would be only a constant amount of shifting.

		Through the execution of the algorithm, once calls to INSERT-INTO-GROUP start exhibiting the second behavior, they will continue to do so for the rest of the algorithm. This is because the max of one group is the min of the next. Additionally, INSERT-INTO-GROUP will take $O(|g|)$ time once, and $O(1)$ time anytime else. Since INSERT INTO GROUP is called $\sqrt{n}$ times, this means INSERT will take $O(\sqrt{n})$ time.

		\item For deletions we assume that the input $x$ is the position in the list of all lists where the element we're removing is. The algorithm is constructed as such:

		\begin{algorithmic}
		\Function{DELETE}{$S$, $x$}
			\If{$x \in S[|S|]$}
				\State Remove $x$ from $S[|S|]$ and shift everything backward one space.
				\State Subtract one from the rotation amount if $x$ was a minimum.
				\State \Return
			\Else
				\State $r \gets S[|S|].\min$
				\State Remove the minimum from $S[|S|]$, shift everything backward.
				\State $i \gets |S|-1$
				\While{$i > 0$}
					\If{$x \in S[i]$}
						\State Replace $x$ with $r$.
						\State Bubble $r$ it until it reaches it's place.
						\State Change the rotation amount of $S[i]$ accordingly.
						\State \Return
					\Else
						\State $m \gets r$
						\State $r \gets S[i].\min$
						\State Replace $S[i].\min$ with $m$.
						\State Subtract one from $S[i]$'s rotation amount.
					\EndIf
				\EndWhile
			\EndIf
		\EndFunction
		\end{algorithmic}
		What the algorithm essentially does is remove the element at position $x$ and replace it with the next group's minimum, doing necessary shifting to keep the current group sorted. It then recurses to the next group and replaces it's missing space with the next group's minimum. This takes constant time. It continues to do this until it reaches the bottom. The pseudo code actually does all of this in reverse, but explaining is better this way.

		This algorithm does at most $\sqrt{n}$ iterations of it's while loop. Only two iterations of this loop will take more than constant time. These iterations take $O(\sqrt{n})$ time. This means the algorithm takes $O(3 \sqrt{n}) = O(\sqrt{n})$ time.

		\item Searching can be done by doing a binary search over each group's max and min. If the search key ``falls between the cracks'' of two groups (i.e.\ for two successive groups $g_1$ and $g_2$ we find $g_1.\max < x < g_2.\min$) then it does not exist in the data structure. If $x \in [g.\min, g.\max]$ for some $g \in S$ then we do a binary search over the elements of $g$, keeping in mind that it's rotated by a set amount.

		Let $g.\text{offset}$ be the group $g$'s rotation amount.
		\begin{algorithmic}
		\Function{SEARCH}{$S$, $x$}
			\State $intervalmin \gets 0$
			\State $intervalmax \gets 2|S|-1$
			\While{$intervalmin < intervalmax$}
				\State $midpoint \gets \floor{(intervalmin + intervalmax)/2}$
				\State $smid \gets \floor{midpoint/2}$
				\State $smidval \gets S[smid+1].\max $
				\If{$midpoint \mod 2 = 0$}
					\State $smidval \gets S[smid+1].\min $
				\EndIf
				\If{$x > smidval$}
					\State $intervalmin \gets midpoint+1$
				\Else
					\State $intervalmax \gets midpoint$
				\EndIf
			\EndWhile
			\If{$intervalmin \neq intervalmax$}
				\State \Return No Key
			\EndIf

			\State $r \gets intervalmin$
			\If{$r \mod 2 == 0$}
				\State \Return No Key
			\EndIf
			\State $r \gets \floor{r/2}$
			\State $g \gets S[r]$


			\State $intervalmin \gets 0$
			\State $intervalmax \gets |g|-1$
			\While{$intervalmin < intervalmax$}
				\State $midpoint \gets \floor{(intervalmin + intervalmax)/2}$
				\State $gmidval \gets g[((midpoint+g.\text{offset}) \mod |g|) + 1] $

				\If{$x > gmidval$}
					\State $intervalmin \gets midpoint+1$
				\Else
					\State $intervalmax \gets midpoint$
				\EndIf
			\EndWhile
			\If{$intervalmin = intervalmax$ and $ $}

				\Return $x$'s data/position
			\EndIf

			\State \Return No Key
		\EndFunction
		\end{algorithmic}

		Since there are at most $\sqrt{n}$ groups and at most $\sqrt{n}$ elements in each group, this algorithm takes $O(\log(\sqrt{n}))+O(\log(\sqrt{n})) = O(1/2 \log(n)) = O(\log(n))$ time.

		\item Implementing this structure as a dictionary is as simple as making each element of each array a pointer to a key-value pair. In comparison to other data structures, consider the following table:

		\begin{tabular}{|l|c|c|c|c|}
		  \hline
		   & Unsorted Ary & Sorted Ary & Red-Black & Rotated Lists \\
		  \hline \hline
		  Insert & $O(1)$ & $O(n)$ & $O(\log(n))$ & $O(\sqrt{n})$ \\
		  \hline
		  Delete & $O(1)$ & $O(n)$ & $O(\log(n))$ & $O(\sqrt{n})$ \\
		  \hline
		  Search & $O(n)$ & $O(\log(n))$ & $O(\log(n))$ & $O(\log(n))$ \\
		  \hline
		\end{tabular}

		Running time for search trumps all of these algorithms. However, insert and delete, although faster than the sorted array, is slower than Red-Black trees. This structure would be a good fit for situations where insertions and deletions are less common than searches.

		It would also be good for large datasets, because in practice the algorithm searches over a sorted list of size $\sqrt{n}$ and then a sorted list of size $n \leq \sqrt{n}$. If keys are ranked in such a way that lower values are searched more often, then $n$ would be small, and the amount of comparisons would be closer to $\log(\sqrt{n}) + \log(c)$ for some constant $c$.

		Red-Black trees don't have this advantage, since low valued keys would be closer to the leftmost side of the tree, and possibly near the bottom. This would make the practical number of comparisons around $\log(n)$ for low valued keys.

	\end{enumerate}

\end{enumerate}
\end{document}